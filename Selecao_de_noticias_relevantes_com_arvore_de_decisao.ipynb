{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Selecao de noticias relevantes com arvore de decisao.ipynb",
      "authorship_tag": "ABX9TyONda+XJx2eTlimXpixJ2yk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaqueline-amorim/Selecao-de-noticias-relevantes-com-arvore-de-decisao/blob/main/Selecao_de_noticias_relevantes_com_arvore_de_decisao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Seleção de notícias relevantes com Árvore de decisão**"
      ],
      "metadata": {
        "id": "D8x1T5fgprQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trabalho teve por objetivo criar uma árvore de decisão que através de uma API (interface de programação de aplicações), encontra notícias conforme o termo de busca, em fontes oficias pré determinadas, classifica os artigos, e os considerados relevantes são apresentados com o titutulo, descrição, URL e fonte."
      ],
      "metadata": {
        "id": "ohU3s6mnRyCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "vNoG7fLooatU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sua chave de API do News API\n",
        "api_key = 'coloque a sua chave aqui'"
      ],
      "metadata": {
        "id": "fvl3RE2vodYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Termo de busca\n",
        "query = 'Enchentes Rio Grande do Sul 2024', 'municípios afetados pelas enchentes Rio Grande do Sul 2024'"
      ],
      "metadata": {
        "id": "k7YvNzYiogB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de domínios de fontes oficiais (exemplos)\n",
        "official_sources = [\n",
        "    'globo.com', 'estadao.com.br', 'folha.uol.com.br', 'g1.globo.com',\n",
        "    'agenciabrasil.ebc.com.br', 'bbc.com', 'brasil.mapbiomas.org',\n",
        "    'estado.rs.gov.br', 'ufrgs.br'\n",
        "]"
      ],
      "metadata": {
        "id": "434ZDErJoi9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para buscar notícias usando News API\n",
        "def search_news_newsapi(query):\n",
        "    url = f'https://newsapi.org/v2/everything'\n",
        "    params = {\n",
        "        'apiKey': api_key,\n",
        "        'q': query,\n",
        "        'language': 'pt',\n",
        "        'domains': ','.join(official_sources),\n",
        "        'pageSize': 50\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f'Erro na requisição para {query}:', response.status_code)\n",
        "        return None"
      ],
      "metadata": {
        "id": "WWfLmrfuomxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso: coleta de dados\n",
        "data = search_news_newsapi(query)\n",
        "\n",
        "if data:\n",
        "    if 'articles' in data:\n",
        "        articles = data['articles']\n",
        "        X = [article['title'] + ' ' + article['description'] for article in articles if 'description' in article]\n",
        "        y = [1 if 'enchentes' in article['title'].lower() or 'enchentes' in article['description'].lower() else 0 for article in articles]\n",
        "\n",
        "        # Dividir dados em treino e teste\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Criar pipeline com vetorização de texto e árvore de decisão\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', CountVectorizer()),  # Vetorização de texto simples\n",
        "            ('classifier', DecisionTreeClassifier(random_state=42))  # Árvore de decisão com seed para reproducibilidade\n",
        "        ])\n",
        "\n",
        "        # Treinar o modelo\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Avaliar o modelo\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Avaliação com validação cruzada\n",
        "        scores = cross_val_score(pipeline, X, y, cv=5)  # cv=5 para 5-fold cross-validation\n",
        "        print(f'Acurácia da validação cruzada: {scores.mean():.2f} (+/- {scores.std() * 2:.2f})')\n",
        "\n",
        "        # Treinar o modelo com todos os dados\n",
        "        pipeline.fit(X, y)\n",
        "\n",
        "        # Exemplo de uso da árvore de decisão treinada para classificar novos artigos\n",
        "        new_articles = [\n",
        "            \"enchentes\",\n",
        "            \"combate às enchentes\",\n",
        "            \"impacto das enchentes\"\n",
        "        ]\n",
        "        new_articles_formatted = [' '.join(new_article.split()) for new_article in new_articles]  # Formatando os novos artigos\n",
        "\n",
        "        predictions = pipeline.predict(new_articles_formatted)\n",
        "\n",
        "        # Exibir resultados\n",
        "        for article, prediction in zip(new_articles, predictions):\n",
        "            if prediction == 1:\n",
        "                print(f'Artigo relevante: {article}')\n",
        "                for a in articles:\n",
        "                    # Verifica se qualquer palavra-chave do novo artigo está contida no título ou descrição do artigo da API\n",
        "                    if any(keyword.lower() in (a['title'] + ' ' + a['description']).lower() for keyword in new_articles):\n",
        "                        print('Título:', a['title'])\n",
        "                        print('Descrição:', a['description'] if 'description' in a else 'N/A')\n",
        "                        print('URL:', a['url'] if 'url' in a else 'N/A')\n",
        "                        print('Fonte:', a['source']['name'] if 'source' in a and 'name' in a['source'] else 'N/A')\n",
        "                        print('-' * 200)\n",
        "            else:\n",
        "                print(f'Artigo não relevante: {article}')\n",
        "    else:\n",
        "        print('Nenhum artigo encontrado.')\n",
        "else:\n",
        "    print('Erro ao obter dados da API.')\n",
        "\n"
      ],
      "metadata": {
        "id": "YXiwSeOrNIIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}